<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Are Discrete Diffusion Models Better Than Auto-regressive Models in Text Generation? Uncovering a Hidden Numerical Issue | Kaiwen Zheng (郑凯文) </title> <meta name="author" content="Kaiwen Zheng (郑凯文)"> <meta name="description" content="With SEDD winning the Best Paper Award at ICML 2024, discrete diffusion models have emerged as a promising contender to auto-regressive models in text generation. In this blog, however, we uncover a hidden yet critical numerical precision issue that negatively impacts generation diversity in discrete diffusion sampling. This flaw highlights the limitations of previous evaluations, which rely solely on the incomplete metric of generative perplexity, resulting in a secretely unfair comparison to auto-regressive models. For complete analyses and proofs, please refer to our paper (http://arxiv.org/pdf/2409.02908)."> <meta name="keywords" content="kaiwen-zheng, academic-website, machine-learning, generative-models"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%A1&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zhengkw18.github.io/blog/2024/mdm/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Are Discrete Diffusion Models Better Than Auto-regressive Models in Text Generation? Uncovering a Hidden Numerical Issue",
            "description": "With SEDD winning the Best Paper Award at ICML 2024, discrete diffusion models have emerged as a promising contender to auto-regressive models in text generation. In this blog, however, we uncover a hidden yet critical numerical precision issue that negatively impacts generation diversity in discrete diffusion sampling. This flaw highlights the limitations of previous evaluations, which rely solely on the incomplete metric of generative perplexity, resulting in a secretely unfair comparison to auto-regressive models. For complete analyses and proofs, please refer to our paper (http://arxiv.org/pdf/2409.02908).",
            "published": "September 12, 2024",
            "authors": [
              
              {
                "author": "Kaiwen Zheng",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Tsinghua University, NVIDIA",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Kaiwen</span> Zheng (郑凯文) </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Are Discrete Diffusion Models Better Than Auto-regressive Models in Text Generation? Uncovering a Hidden Numerical Issue</h1> <p>With SEDD winning the Best Paper Award at ICML 2024, discrete diffusion models have emerged as a promising contender to auto-regressive models in text generation. In this blog, however, we uncover a hidden yet critical numerical precision issue that negatively impacts generation diversity in discrete diffusion sampling. This flaw highlights the limitations of previous evaluations, which rely solely on the incomplete metric of generative perplexity, resulting in a secretely unfair comparison to auto-regressive models. For complete analyses and proofs, please refer to our paper (http://arxiv.org/pdf/2409.02908).</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <ul> <li> <a href="#masked-diffusion-models-as-the-best-performing-discrete-diffusion">Masked Diffusion Models as the Best-Performing Discrete Diffusion</a> </li> </ul> <div> <a href="#does-lower-generative-perplexity-indicate-better-quality">Does Lower Generative Perplexity Indicate Better Quality?</a> </div> <ul> <li> <a href="#token-diversity-matters">Token Diversity Matters</a> </li> <li> <a href="#trade-off-between-generative-perplexity-and-entropy">Trade-Off between Generative Perplexity and Entropy</a> </li> </ul> <div> <a href="#what-is-the-root-cause-of-reduced-diversity">What is the Root Cause of Reduced Diversity?</a> </div> <ul> <li> <a href="#identifying-the-numerical-precision-issue">Identifying the Numerical Precision Issue</a> </li> <li> <a href="#theoretical-explanations">Theoretical Explanations</a> </li> </ul> <div> <a href="#concluding-remarks">Concluding Remarks</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>In this section, we provide a brief and intuitive overview of both continuous and discrete score-based diffusion models. We recommend referring to <a href="https://yang-song.net/blog/2021/score/#connection-to-diffusion-models-and-others" rel="external nofollow noopener" target="_blank">Yang Song’s blog</a> and <a href="https://aaronlou.com/blog/2024/discrete-diffusion/" rel="external nofollow noopener" target="_blank">Aaron Lou’s blog</a> for more in-depth explanations.</p> <p><em>Likelihood-based</em> probabilistic generative models<d-footnote>Typical likelihood-based models include autoregressive models, normalizing flow models, energy-based models, and variational auto-encoders. Diffusion models, both continuous and discrete, are also likelihood-based.</d-footnote> parameterize a density network $p_\theta$ to learn the data distribution $p_{\text{data}}$. The data space $\mathcal X$ can be either continuous (like $\mathbb R^d$) or discrete (like $\mathcal V^d$ for vocabulary $\mathcal V$), where we use $d$ to denote the data dimension. The model can be trained by maximizing the log-likelihood $\mathbb E_{x \sim p_{\text{data}}} \left[ \log p_\theta(x) \right]$, and samples can be generated by drawing from $p_\theta$.</p> <p>Training $p_\theta$ faces two major challenges:</p> <ul> <li>$p_\theta$ must be normalized over all states, i.e., $\int_{\mathcal X}p_\theta(x)\mathrm d x=1$ (for continuous data) or $\sum_{x\in\mathcal X} p_\theta(x)=1$ (for discrete data). The former involves an intractable integral, and the latter involves a total number $|\mathcal V|^d$ of states exponential to the data dimension.</li> <li>High-dimensional data distributions are typically sparse and hard to fit (curse of dimensionality).</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/mdm/perturb_sde-480.webp 480w,/assets/img/blog/mdm/perturb_sde-800.webp 800w,/assets/img/blog/mdm/perturb_sde-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/mdm/perturb_sde.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Score-based diffusion models<d-cite key="song2020score"></d-cite> address the first challenge by learning the <strong>score function</strong> $\nabla_x\log p_{\text{data}}(x)$ which cancels out the normalizing constant, and address the second challange by modeling a series of noise-perturbed distributions $\{p_t\}_{t\in[0,1]}$. In the continuous-time limit, the forward diffusion process can be described as a stochastic process, with the final distribution being approximately Gaussian, making both learning and sampling manageable. After learning the time-dependent score $\nabla_x\log p_t(x)$, the forward diffusion process can be reversed to approximately draw samples from the data distribution.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/mdm/uniform-480.webp 480w,/assets/img/blog/mdm/uniform-800.webp 800w,/assets/img/blog/mdm/uniform-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/mdm/uniform.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/mdm/absorbing-480.webp 480w,/assets/img/blog/mdm/absorbing-800.webp 800w,/assets/img/blog/mdm/absorbing-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/mdm/absorbing.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Discrete forward diffusion process of a single token. Left: uniform. Right: absorbing (or masked). </div> <p>Discrete diffusion models can be defined in a similar score-based continuous-time approach<d-cite key="sun2022score,lou2023discrete"></d-cite>. For the case of single dimension ($d=1$), the forward discrete diffusion process is described by a continuous-time Markov chain (CTMC), where the token randomly transits according to some predefined rate matrix $Q_t$. The evolution of the <strong>marginal distribution</strong> $p_t$ of the token $x_t$ at time $t$ follows the Kolmogorov forward equation $\frac{\mathrm d p_t}{\mathrm d t}= p_t Q_t$. The forward process can be chosen as <strong>uniform</strong> or <strong>absorbing (or masked)</strong>, so that $p_t$ converges to a uniform stationary distribution or a concentration on an additionally added mask token <code class="language-plaintext highlighter-rouge">[M]</code>.</p> <p>In contrast to the continuous case, the score function $\nabla_x\log p_t(x)$ is not applicable as there is no proper gradient in the discrete space. Instead, the model can learn the probability ratio $\frac{p_t(y)}{p_t(x)}$ between different tokens $x$ and $y$, which is known as <strong>concrete score</strong><d-cite key="meng2022concrete"></d-cite> and also eliminates the normalizing constant. Recently, SEDD<d-cite key="lou2023discrete"></d-cite> proposes the score entropy as a scalable and robust objective for learning the concrete score. With the learned concrete score, the discrete forward process can also be approximately reversed for sampling.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/mdm/multidim-480.webp 480w,/assets/img/blog/mdm/multidim-800.webp 800w,/assets/img/blog/mdm/multidim-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/mdm/multidim.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> The model predicts the probability ratio between neighboring sequences which differ by 1 token. </div> <p>In the multi-dimensional case ($d&gt;1$), the number of possible states $|\mathcal V|^d$ grows exponentially with the data dimension (e.g., $50527^{1024}$ for sequences of length 1024 using <code class="language-plaintext highlighter-rouge">GPT-2</code> tokenizer), and it is computationally intractable to model transitions between two arbitrary states. Instead, the model only predicts probabilities of <em>single-token change</em>. Besides, both the forward and reverse processes are factorized across dimensions, where all dimensions undergo transitions simultaneously and independently (except that the network is conditional on all dimensions).</p> <h3 id="masked-diffusion-models-as-the-best-performing-discrete-diffusion">Masked Diffusion Models as the Best-Performing Discrete Diffusion</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/mdm/forward-480.webp 480w,/assets/img/blog/mdm/forward-800.webp 800w,/assets/img/blog/mdm/forward-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/mdm/forward.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/mdm/backward-480.webp 480w,/assets/img/blog/mdm/backward-800.webp 800w,/assets/img/blog/mdm/backward-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/mdm/backward.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Forward noising and reverse sampling processes of masked diffusion models. </div> <p>Empirically, the absorbing (or masked) variant demonstrates superior performance over other discrete diffusion schedules such as uniform, and is referred to as <strong>masked diffusion models (MDMs)</strong>. This can be attributed to the simple masked mechanism: transitions are sparse and only happen once between data tokens and the mask token <code class="language-plaintext highlighter-rouge">[M]</code> in the whole generation process, which are relatively easier to predict. In some recent works<d-cite key="shi2024simplified,sahoo2024simple"></d-cite>, the masked diffusion formulation is further simplified to a mean-prediction model $\mu_\theta$ with simple weighted cross-entropy training objectives, bringing empirical improvements.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/mdm/network-480.webp 480w,/assets/img/blog/mdm/network-800.webp 800w,/assets/img/blog/mdm/network-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/mdm/network.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Auto-regressive models with causal attention, and masked models with bi-directional attention. </div> <p>Under mean-parameterization, MDMs become quite similar to typical <strong>masked models</strong><d-footnote>Masked models can be applied to both representation learning (such as BERT<d-cite key="devlin2019bert"></d-cite>, MAE<d-cite key="he2022masked"></d-cite>) and generative modeling (such as Mask-Predict<d-cite key="ghazvininejad2019mask"></d-cite>, MaskGIT<d-cite key="chang2022maskgit"></d-cite>)</d-footnote> that learn to reconstruct masked tokens. The key difference is that, MDMs utilize network architectures, training objectives and sampling procedures that <em>rely on the continuous time variable</em>. We illustrate the sampling step in MDMs as follows:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/mdm/sampling-480.webp 480w,/assets/img/blog/mdm/sampling-800.webp 800w,/assets/img/blog/mdm/sampling-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/mdm/sampling.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Illustration of the sampling step in masked diffusion models. </div> <p>Specifically, let $\mathbf x_t=x_t^{(1)}x_t^{(2)}\cdots x_t^{(d)}$ represent the sequence at time $t$. For each position $i$ satisfying $x_t^{(i)}=\text{[M]}$, the transition from time $t$ to time $s&lt;t$ is performed by sampling $x_s^{(i)}\sim\text{Cat}(\pi^{(i)})$, where $\text{Cat}$ denotes the <strong>categorical distribution</strong> and $\mathbf{\pi}^{(i)}=p_{t\rightarrow s}^{\text{remain}}\mathbf e_{\text{[M]}}+(1-p_{t\rightarrow s}^{\text{remain}})\mu_\theta^{(i)}$. Here, $\mathbf e_{\text{[M]}}$ denotes the one-hot vector for the mask token, and the remaining probability $p_{t\rightarrow s}^{\text{remain}}$ is independent of the network output $\mu_\theta$. In each sampling step of MDMs, whether a masked token will be unmasked is determined by rolling a dice (i.e., categorical sampling), which is distinguished from the token-by-token decoding process of masked models. The number of sampling steps in MDMs can be larger than the sequence length $d$, and a single sampling step can result in no token changes.</p> <h2 id="does-lower-generative-perplexity-indicate-better-quality">Does Lower Generative Perplexity Indicate Better Quality?</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/mdm/sedd-480.webp 480w,/assets/img/blog/mdm/sedd-800.webp 800w,/assets/img/blog/mdm/sedd-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/mdm/sedd.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/mdm/md4-480.webp 480w,/assets/img/blog/mdm/md4-800.webp 800w,/assets/img/blog/mdm/md4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/mdm/md4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/mdm/mdlm-480.webp 480w,/assets/img/blog/mdm/mdlm-800.webp 800w,/assets/img/blog/mdm/mdlm-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/mdm/mdlm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Trade-off betweem generative perplexity and the number of sampling steps. Figures taken from SEDD<d-cite key="lou2023discrete"></d-cite>, MD4<d-cite key="shi2024simplified"></d-cite>, MDLM<d-cite key="sahoo2024simple"></d-cite> respectively. </div> <p>Generative perplexity (Gen PPL) is the main metric in previous works to evaluate the generation quality. Specifically, it measures the likelihood of generated text under some off-the-shelf model (typically <code class="language-plaintext highlighter-rouge">GPT-2 Large</code>). Lower Gen PPL means larger probability of the generated sample.</p> <p>As suggested by multiple previous works<d-cite key="lou2023discrete,shi2024simplified,sahoo2024simple"></d-cite>, it can be observed that the Gen PPL continues to decrease as the number of sampling steps increases. When the number of sampling steps reaches around 2,000, the Gen PPL of MDMs can even surpass that of counterpart auto-regressive models (ARMs). At first glance, this seems reasonable, as the trade-off between sample quality and inference speed is a key characteristic of diffusion models.</p> <p>However, we argue that Gen PPL is not comprehensive for evaluating the generation quality of text. Unlike the Fréchet inception distance (FID) metric for images, which compares the whole distribution of generated images with that of real images, Gen PPL only favors high-probability samples, while neglecting other features like diversity and mode coverage.</p> <h3 id="token-diversity-matters">Token Diversity Matters</h3> <p>We present two samples from ARMs and MDMs to demonstrate the diversity problem.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;|endoftext|&gt; the new cars are crossovers.

AT&amp;T Insurance Marketing Manager, Megan Maxwell, tells us that Model X was "reasonably priced, effective and inspires strong sentiment among consumers." She says:

Our GM car for discussion is shown as part of our drive 20 percent around the world and even a competitor. Our GM for discussion alt shows as one of our most popular cars in the world. We are in multiple countries introducing firmware for our new vehicles. While we are confident in our prices, we rely upon GM Auto’s sales data and know we must adapt this process to meet the needs of all customers.

The proposed pricing is similar to that of the cheaper Range Rover and other cheaper sport utility vehicles, which are primarily offered through its dealerships. Alongside a Volt, Delphi XE8 includes a plug-in hybrid version called Volt Energy.

"Dynamic pricing is our way to deliver owners of more attractive or more reasonable outcomes or to find more marketable models that appeal to them more than their competitors," notes Maxwell.

Earlier this week, GM analyst Greg Clifford predicted that Intel Global Radical Charge Power Savings (STB) would start at $3,300 over the product lifecycle with an adoption rate of 50 percent by 2025.&lt;|endoftext|&gt;
</code></pre></div></div> <div class="caption"> A sample from auto-regressive models, Gen PPL$\approx$ 30. </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;|endoftext|&gt; I was striking out, he got it to the third out. I know it wasn’t really a catch there, but he didn’t consider that at all. I knew what it wasn’t about. We scored by one run, so it was a difficult thing for me to go to a “get-out,” but being on the board is not an easy fact. I knew it was in play, and went to the third out that it needed to be thrown to. I had nothing to say about it, and it was the only opportunity for me to get, to make an appeal that might not come back, and it was the only time he saw that at that moment.

And so we continued.

“So we were sitting in the plus-five-and-plus-three-at-night, because we were up on an inning. We were in pinch-hit early in the ninth. I looked at a guy and was very nervous and confused, so when I looked at this man — I can’t remember what he was doing, but to double or dead-se the bases for sure — I said you’ll be the first one up, then I’m out of the ballpark. I felt like something had to be wrong with me, and he turned to me and told me, “that’s as bad as the rest,” just like that — “I bet!” I said calmly. “I bet,’” “Gotta do your best! Do not bet!’” “And when that happens, do you think I can be won by a one or two runs?” He stood there and looked stunned. “You mean that?” “Yes, absolutely.” “‘Yeah, absolutely.” “Well, is that a message to you?’“” And he looked back to me and said, “So we aren’t going out?” “Well, yes,” said me, “but I believe so I believe.”” “I bet,” “Yeah, I bet, but when we’re on the board, how much time are we gonna lose?” “Absolutely not,” “I bet,” “all right. I’m not going out. It’s me, understand.” “I bet, I believe not.” “What happened?”” “It happened!” “Did you hear a clue?” “I said, “Oh, no! I-I-I heard that fifth-dinger! Give me the clue!” The players, myself, and the “Man, Man, Man, it’s just beyond hell!” murmurs of the players. At the same time, I said, myself, “Young man, I’ve got to say — I won’t screw you right here.” I went on, “You can take it. You’re not going to lose.”” I smiled. “You know what you got to admit to yourself? This happened in baseball. I didn’t screw you in baseball, it don’t matter, I’ll screw you in a way.” He took the fifth-dinger and said, “It’s the end in baseball, it’s the end.” “That’s correct!” I said. “Yes, you can’t win in baseball,” I said. “But you’re not winning in baseball.” He turned to me. “No, really, it’s alright whether you’re winning or not.” — “I’m sure,” he said. “Good money!” — I cut off. “You’re not going to get this out. You do.” Those were a few words. As I were thinking, “What an enterprise.”

“What are you in baseball?”

“Ah, and it’s a game, not a story and a number. If you believe the most in-the-30s stories are about the when-they-had-to-be-done-as-cardinals-but-recan’t-they-get-in story?” I said “suck,” and “We did stop listening to the number, and we had to come off with the number.”

“Exactly,” said Mike. &lt;|endoftext|&gt;
</code></pre></div></div> <div class="caption"> A sample from masked diffusion models with 50k sampling steps, Gen PPL$\approx$ 10. </div> <p>With as many as 50,000 sample steps, MDMs can reach an extremely low Gen PPL. However, repetitive patterns such as “I bet!” and “I said” frequently appear, diminishing the diversity of tokens in the sequence.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/mdm/fp32_gen_ppl-480.webp 480w,/assets/img/blog/mdm/fp32_gen_ppl-800.webp 800w,/assets/img/blog/mdm/fp32_gen_ppl-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/mdm/fp32_gen_ppl.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/mdm/fp32_gen_entro-480.webp 480w,/assets/img/blog/mdm/fp32_gen_entro-800.webp 800w,/assets/img/blog/mdm/fp32_gen_entro-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/mdm/fp32_gen_entro.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Gen PPL and sentence entropy of SEDD Absorb<d-cite key="lou2023discrete"></d-cite> and MDLM<d-cite key="sahoo2024simple"></d-cite>, varying the number of sampling steps in {100,500,1000,5000,10000}. </div> <p>To quantitatively assess the token diversity, we additionally measure the <strong>sentence entropy</strong><d-footnote>For a sequence of length $L$ that contains $K$ distinct tokens, with each token $k$ occurring $L_k$ times, the entropy is computed as $-\sum_{k=1}^K p_k \log p_k$, where $p_k = L_k/L$ represents the probability of occurrence of token $k$.</d-footnote>. We find that the entropy of MDMs, both in the absorbing case of SEDD<d-cite key="lou2023discrete"></d-cite> and in its later improved version MDLM<d-cite key="sahoo2024simple"></d-cite>, is consistently lower than that of ARMs and continues to decrease with more sampling steps.</p> <h3 id="trade-off-between-generative-perplexity-and-entropy">Trade-Off between Generative Perplexity and Entropy</h3> <div style="text-align: center;"> <img src="/assets/img/blog/mdm/tradeoff.png" class="img-fluid rounded z-depth-1" width="50%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </div> <div class="caption"> Trade-off curve of Gen PPL and entropy in MDMs and ARMs. </div> <p>Our observations reveal that varying the number of sampling steps in MDMs creates an inherent trade-off between Gen PPL and entropy. This effectively changes the temperature, leading to an unfair comparison with ARMs, which are not subject to temperature scaling. After manually adjusting the temperature for ARMs to ensure a fair comparison at the same entropy level, we find that the Gen PPL of MDMs falls significantly behind.</p> <h2 id="what-is-the-root-cause-of-reduced-diversity">What is the Root Cause of Reduced Diversity?</h2> <p>The reduced token diversity and low generation quality is unexpected. In theory, increasing the number of sampling steps should reduce discretization errors and more accurately reflect the true model performance, as already seen in continuous diffusion models. We therefore consider this an implementation issue and investigate further to identify the root cause.</p> <h3 id="identifying-the-numerical-precision-issue">Identifying the Numerical Precision Issue</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/mdm/fp64_gen_ppl-480.webp 480w,/assets/img/blog/mdm/fp64_gen_ppl-800.webp 800w,/assets/img/blog/mdm/fp64_gen_ppl-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/mdm/fp64_gen_ppl.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/mdm/fp64_gen_entro-480.webp 480w,/assets/img/blog/mdm/fp64_gen_entro-800.webp 800w,/assets/img/blog/mdm/fp64_gen_entro-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/mdm/fp64_gen_entro.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Gen PPL and sentence entropy with 64-bit categorical sampling.. </div> <p>Surprisingly, we find that by simply altering the floating-point precision during sampling from 32-bit to 64-bit, the entropy returns to a normal level similar to ARMs (5.6~5.7), but with a generative perplexity $\approx100$. After careful ablations, we identify the root cause as the <strong>numerical inaccuracy in previous Gumbel-based categorical sampling</strong>.</p> <p>Denote $\mathcal U(0,1)$ as the uniform distribution on $[0,1]$, and $\mathcal G(0,1)$ as the standard Gumbel distribution<d-footnote>https://en.wikipedia.org/wiki/Gumbel_distribution</d-footnote>. To sample from a categorical distribution with class probabilities $\pi=[\pi_1\ \pi_2\ \cdots\ \pi_K]$, Gumbel-max trick<d-footnote>An introduction to the Gumbel-max trick can be found at https://homes.cs.washington.edu/~ewein//blog/2022/03/04/gumbel-max/.</d-footnote> is used by first sampling $K$ independent uniform variables $u_i\sim\mathcal U(0,1)$, then transforming them into samples from $\mathcal G(0,1)$ by $g_i=-\log(-\log u_i)$, and finally obtaining the categorical sample $n=\arg\max_{i} (\log\pi_i+g_i)$.</p> \[u_i\sim\mathcal U(0,1)\Rightarrow g_i=-\log(-\log u_i)\sim \mathcal G(0,1)\Rightarrow\arg \max_i(\log\pi_i+g_i)\sim \text{Cat}(\pi)\] <p>The operation $g=-\log(-\log u)$ theoretically maps $u\in[0,1]$ to $g\in(-\infty,+\infty)$. But due to the limited representation ability of floating-point numbers in implementation, $u$ is constrained to $[0,1-\epsilon]$ and $g$ is constrained to $(-\infty,M]$ where $M=-\log(-\log (1-\epsilon))$<d-footnote>For a floating-point format where the fraction part has $f$ bits, $\epsilon$ can be calculated as $2^{-f-1}$. For example, the 32-bit floating point precision corresponds to $f=23,1-\epsilon\approx 0.9999999404,M\approx 16.6355$.</d-footnote>. Therefore, the sample $g$ instead follows a <strong>truncated Gumbel distribution</strong>, denoted $\mathcal T \mathcal G(0,1,M)$, which refers to the Gumbel distribution $\mathcal G(0,1)$ conditioned on $g\leq M$. This tricky difference theoretically makes the categorical sampling inaccurate, i.e., $\arg\max_{i} (\log\pi_i+g_i)$ no longer follows the class probabilities $\pi$.</p> \[u_i\sim\mathcal U(0,1-\epsilon)\Rightarrow g_i=-\log(-\log u_i)\sim \mathcal T\mathcal G(0,1,M)\Rightarrow\arg \max_i(\log\pi_i+g_i)\nsim \text{Cat}(\pi)\] <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/mdm/code-480.webp 480w,/assets/img/blog/mdm/code-800.webp 800w,/assets/img/blog/mdm/code-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/mdm/code.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Code for different versions of Gumbel-based categorical sampling. The operation $\arg\max_i(\log\pi_i-\log(-\log u_i))$ is simplified to $\arg\max_i(\pi_i/(-\log u_i))$ to save computation cost. </div> <p>To verify that truncation is the fundamental issue, we conduct ablations by only modifying the categorical sampling code. We manually scale 64-bit uniform samples to match the truncation in the 32-bit case. We then randomly generate 8 samples with 2048 steps and compare the average Gen PPL and entropy.</p> <table> <thead> <tr> <th>Version</th> <th>Gen PPL</th> <th>Entropy</th> </tr> </thead> <tbody> <tr> <td><code class="language-plaintext highlighter-rouge">32-bit</code></td> <td>31.24</td> <td>5.17</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">64-bit</code></td> <td>126.11</td> <td>5.66</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">64-bit + truncation</code></td> <td>28.64</td> <td>5.12</td> </tr> </tbody> </table> <p>For both Gen PPL and entropy, <code class="language-plaintext highlighter-rouge">32-bit</code>$\approx$<code class="language-plaintext highlighter-rouge">64-bit + truncation</code>, which confirms the impact of truncation.</p> <h3 id="theoretical-explanations">Theoretical Explanations</h3> <p>Through further derivation, we are surprised to find that the effect of truncation can be precisely described in closed-form. Specifically, suppose the original class probabilities are sorted as $\pi_1\leq\pi_2\leq\cdots\leq\pi_K$. With the truncated Gumbel distribution $\mathcal T\mathcal G(0,1,M)$, the resulting categorical samples instead follow <strong>shifted class probabilities</strong> $\pi '$:</p> \[\pi_n'=\pi_n\sum_{i=1}^{n}\beta(i),\quad \text{where } \beta(i)\geq 0\] <p>To the best of knowledge, such formulations are revealed for the first time by us.</p> <details><summary>Click here to see the expression of $\beta(i)$</summary> \[\beta(i)=\frac{e^{\left(K+1-i-\frac{\sum_{k=i}^K\pi_k}{\pi_i}\right) e^{-M}}-e^{\left(K+1-i-\frac{\sum_{k=i}^K\pi_k}{\pi_{i-1}}\right) e^{-M}}}{\sum_{k=i}^K\pi_k}\geq 0\] </details> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/mdm/shifted-480.webp 480w,/assets/img/blog/mdm/shifted-800.webp 800w,/assets/img/blog/mdm/shifted-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/mdm/shifted.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>This has two main implications:</p> <ul> <li>As $\beta(i)\geq 0$ and $\pi_n$ are sorted, if $\pi_{n_1}&gt;\pi_{n_2}$, the adjusted class probabilities satisfy $\frac{\pi_{n_1}'}{\pi_{n_2}'}&gt;\frac{\pi_{n_1}}{\pi_{n_2}}$. This indicates that relatively larger probabilities are further amplified. Therefore, for each position in the sequence, the resulting categorical distribution is <strong>biased towards classes with higher probabilities</strong>, creating an effect similar to <strong>lowering the temperature</strong>.</li> <li>As $\pi$ include the probability of remaining as <code class="language-plaintext highlighter-rouge">[M]</code>, different network outputs lead to varying shifting effects, resulting in unequal remaining probability for masked tokens at different positions. Therefore, some mask tokens are <strong>prioritized to be unmasked</strong>, thereby reducing randomness.</li> </ul> <p>Both factors reduce the diversity and lower the entropy. However, we find that 32-bit categorical sampling produces similar results to 64-bit in token-by-token decoding procedures of ARMs and masked models, suggesting that the first factor is relatively insignificant. In contrast, the distinctive reverse-time sampling procedure of MDMs also suffers from prioritized unmasking. The temperature-lowering effects accumulate across numerous sampling steps, eventually leading to notable diversity issues, even under 32-bit floating-point precision.</p> <h2 id="concluding-remarks">Concluding Remarks</h2> <p>In this blog, we illustate how previous works on masked diffusion models secretely suffer from numerical precision issues, leading to somewhat unfair evaluations and doubtful claims. This blog is partially from our paper on <a href="https://arxiv.org/pdf/2409.02908" rel="external nofollow noopener" target="_blank">arXiv</a>, where we additionally prove that <strong>masked diffusion models are exactly equivalent to masked models in both training and sampling</strong>, except for some minor aspects like the loss weighting. Our investigation suggests:</p> <ul> <li>Generative perplexity alone cannot comprehensively reflect the text generation quality. In practice, it better to use the trade-off curve between generative perplexity and sentence entropy for a more holistic evaluation.</li> <li>Masked diffusion models are essentially performing maximum likelihood training of masked models. According to our practice, masked diffusion models, along with other discrete diffusion variants, are far from beating the auto-regressive paradigm in text generation.</li> </ul> <p>Despite our negative findings, we acknowledge that the text-based experiments may inherently favor ARMs, as text naturally follows a left-to-right order that ARMs are better suited to model. Recent works on masked modeling of images<d-cite key="li2024autoregressive,xie2024show"></d-cite> suggest that the masked mechanism could offer advantages over autoregressive next-token prediction in other modalities. In such cases, maximum likelihood training is often unnecessary for achieving good generation quality, and we can directly use masked models like MaskGIT<d-cite key="chang2022maskgit"></d-cite> instead of discrete diffusion models, as they are equivalent to the best discrete diffusion variant while offering simpler formulations.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/mdm.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"zhengkw18/zhengkw18.github.io","data-repo-id":"R_kgDOMtWuQw","data-category":"Announcements","data-category-id":"DIC_kwDOMtWuQ84CidxY","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Kaiwen Zheng (郑凯文). Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>